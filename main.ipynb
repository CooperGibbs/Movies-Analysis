{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "#PYSPARK_DRIVER_PYTHON = 3.85\n",
    "#PYSPARK_PYTHON = 3.85\n",
    "import os\n",
    "import sys\n",
    "#import pyspark as spark\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Same code as shown in SimpleApp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_with_spark(log_file=\"data/movies.csv\", app_name=\"movieAnalysis\"):\n",
    "    spark = SparkSession.builder.config(\"spark.executor.memory\", \"2g\").config(\"spark.driver.memory\", \"2g\").config(\"spark.storage.memoryFraction\", \"0.6\").appName(app_name).getOrCreate()\n",
    "    df = spark.read.option(\"header\",True).csv(log_file).cache()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\"\"\"\n",
    "1. Summary statistics for each relevant data frame\n",
    "\n",
    "\"\"\"\n",
    "def summary_statistics(df: pyspark.sql.dataframe.DataFrame, cols= [\"*\"], stats = [\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]):\n",
    "    df.select(*cols).summary(*stats).show()\n",
    "# 1. Do all\n",
    "def summary_statistics_complete(file= \"data/ratings.csv\", cols= [\"*\"], stats = [\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]):\n",
    "    summary_statistics(open_with_spark(file), cols, stats)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "2. Join Dataframes READABLE version, more memory needed\n",
    "\n",
    "\"\"\"\n",
    "def join_ratings_and_movies_readable():  \n",
    "    df_movie = open_with_spark()\n",
    "    df_ratings = open_with_spark(log_file=\"data/ratings.csv\", app_name=\"ratings\")\n",
    "    df_join = df_ratings.join(df_movie, \"movieID\")\n",
    "    return df_join\n",
    "    \n",
    "# 2. Join Dataframes\n",
    "def join_ratings_and_movies(): \n",
    "    return open_with_spark().join(open_with_spark(log_file=\"data/ratings.csv\", app_name=\"ratings\"), \"movieID\")\n",
    "\n",
    "\"\"\"\n",
    "3. Most-rated movies\n",
    "\n",
    "Returns the top N movies with the most reviews (ratings)\n",
    "\n",
    "\"\"\"\n",
    "# 3. Most-rated movies\n",
    "def most_rated(df: pyspark.sql.dataframe.DataFrame, N=10):\n",
    "    return df.groupby(\"title\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\").sort(\"Num_ratings\", ascending=False).limit(N)\n",
    "\n",
    "# 3. Function that does everything; can be used for timing or outputting or whatever\n",
    "def most_rated_complete(N=10):\n",
    "    JOINED = join_ratings_and_movies()\n",
    "    TOP_N = most_rated(JOINED, N)\n",
    "    return TOP_N\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "4. Highest-average-rated movies\n",
    "\n",
    "Returns the top N movies with the highest average reviews (ratings)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 4. Highest-average-rated movies\n",
    "def best_average_rated(df: pyspark.sql.dataframe.DataFrame, N=10, MIN_RATINGS=50):\n",
    "    T = df.groupby(\"title\").agg(F.mean(\"rating\"))\n",
    "    H = T.withColumnRenamed(\"avg(rating)\", \"Mean_rating\")\n",
    "    # H.show(5)\n",
    "    G = df.groupby(\"title\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\")\n",
    "    C = H.join(G, \"title\")\n",
    "    C = C.filter(C.Num_ratings >= MIN_RATINGS).select([\"title\", \"Mean_rating\"])\n",
    "    J = C.sort(\"Mean_rating\", ascending=False)\n",
    "    K = J.limit(N).withColumn(\"Mean_rating\", F.round(\"Mean_rating\",3))\n",
    "    return K\n",
    "\n",
    "# 4. Function that does everything; can be used for timing or outputting or whatever\n",
    "def best_average_rated_complete(N=10, MIN_RATINGS=50):\n",
    "    JOINED = join_ratings_and_movies()\n",
    "    TOP_N = best_average_rated(JOINED, N, MIN_RATINGS)\n",
    "    return TOP_N \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "5. Popular genres: \n",
    "\n",
    "Find the top N popular genres by calculating the average rating for each genre.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def popular_genres(df: pyspark.sql.dataframe.DataFrame, N=5, MIN_RATINGS=10):\n",
    "    L = df.groupby(\"genres\").agg(F.mean(\"rating\"))\n",
    "    L.show(5)\n",
    "    M = L.withColumnRenamed(\"avg(rating)\", \"Mean_rating\")\n",
    "    G = df.groupby(\"genres\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\")\n",
    "    M.show(5)\n",
    "    K = M.join(G, \"genres\")\n",
    "    P = K.filter(K.Num_ratings >= MIN_RATINGS).select([\"genres\", \"Mean_rating\"])\n",
    "    N = P.sort(\"Mean_rating\", ascending=False).limit(N).withColumn(\"Mean_rating\", F.round(\"Mean_rating\",3))\n",
    "    return N\n",
    "# 5. Retrives DF then computes Top N DF\n",
    "def popular_genres_complete(N=5, MIN_RATINGS=10):\n",
    "    JOINED = join_ratings_and_movies()\n",
    "    TOP_N = popular_genres(JOINED, N)\n",
    "    return TOP_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 17:53:18 WARN Utils: Your hostname, Anthonys-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.67.104.86 instead (on interface en0)\n",
      "23/04/18 17:53:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 17:53:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+-------+-----+\n",
      "|summary|title|\n",
      "+-------+-----+\n",
      "|  count|58098|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 17:53:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "23/04/18 17:53:34 WARN MemoryStore: Not enough space to cache rdd_35_5 in memory! (computed 54.7 MiB so far)\n",
      "23/04/18 17:53:34 WARN BlockManager: Persisting block rdd_35_5 to disk instead.\n",
      "23/04/18 17:53:34 WARN MemoryStore: Not enough space to cache rdd_35_1 in memory! (computed 54.3 MiB so far)\n",
      "23/04/18 17:53:34 WARN BlockManager: Persisting block rdd_35_1 to disk instead.\n",
      "23/04/18 17:53:34 WARN MemoryStore: Not enough space to cache rdd_35_7 in memory! (computed 54.6 MiB so far)\n",
      "23/04/18 17:53:34 WARN BlockManager: Persisting block rdd_35_7 to disk instead.\n",
      "23/04/18 17:53:34 WARN MemoryStore: Not enough space to cache rdd_35_3 in memory! (computed 54.4 MiB so far)\n",
      "23/04/18 17:53:34 WARN BlockManager: Persisting block rdd_35_3 to disk instead.\n",
      "23/04/18 17:53:34 WARN MemoryStore: Not enough space to cache rdd_35_6 in memory! (computed 54.7 MiB so far)\n",
      "23/04/18 17:53:34 WARN BlockManager: Persisting block rdd_35_6 to disk instead.\n",
      "23/04/18 17:53:34 WARN MemoryStore: Not enough space to cache rdd_35_4 in memory! (computed 54.6 MiB so far)\n",
      "23/04/18 17:53:34 WARN BlockManager: Persisting block rdd_35_4 to disk instead.\n",
      "23/04/18 17:53:37 WARN MemoryStore: Not enough space to cache rdd_35_6 in memory! (computed 22.6 MiB so far)\n",
      "23/04/18 17:53:37 WARN MemoryStore: Not enough space to cache rdd_35_4 in memory! (computed 54.6 MiB so far)\n",
      "23/04/18 17:53:37 WARN MemoryStore: Not enough space to cache rdd_35_3 in memory! (computed 54.4 MiB so far)\n",
      "23/04/18 17:53:37 WARN MemoryStore: Not enough space to cache rdd_35_1 in memory! (computed 6.6 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|summary|userID|\n",
      "+-------+------+\n",
      "|  count|283228|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Print out number of movies and number of unqiue users.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "summary_statistics_complete(\"data/movies.csv\",[\"title\"], [\"count\"])\n",
    "# Get df of unique users\n",
    "df_unique_users = open_with_spark(\"data/ratings.csv\").select(\"UserID\").distinct()\n",
    "summary_statistics(df_unique_users,[\"userID\"], [\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/17 14:41:22 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/17 14:41:22 WARN MemoryStore: Not enough space to cache rdd_35_0 in memory! (computed 22.6 MiB so far)\n",
      "23/04/17 14:41:22 WARN MemoryStore: Not enough space to cache rdd_35_2 in memory! (computed 35.2 MiB so far)\n",
      "23/04/17 14:41:22 WARN MemoryStore: Not enough space to cache rdd_35_1 in memory! (computed 35.3 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            rating|\n",
      "+-------+------------------+\n",
      "|  count|          27753444|\n",
      "|   mean|3.5304452124932677|\n",
      "| stddev| 1.066352750231989|\n",
      "|    min|               0.5|\n",
      "|    25%|               3.0|\n",
      "|    50%|               3.5|\n",
      "|    75%|               4.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary_statistics_complete(\"data/ratings.csv\",[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 17:33:23 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/18 17:33:23 WARN MemoryStore: Not enough space to cache rdd_35_0 in memory! (computed 22.6 MiB so far)\n",
      "23/04/18 17:33:23 WARN MemoryStore: Not enough space to cache rdd_35_1 in memory! (computed 35.3 MiB so far)\n",
      "23/04/18 17:33:23 WARN MemoryStore: Not enough space to cache rdd_35_6 in memory! (computed 35.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o79.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:393)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:402)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:424)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3709/0x0000000801f66cf8.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset$$Lambda$3660/0x0000000801f2d168.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.Dataset$$Lambda$2086/0x0000000801b944f0.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset$$Lambda$1753/0x0000000801abd458.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1764/0x0000000801ac0d48.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1754/0x0000000801abd718.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeVirtual(DirectMethodHandle$Holder)\n\tat java.base/java.lang.invoke.LambdaForm$MH/0x0000000801160400.invoke(LambdaForm$MH)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m R \u001b[39m=\u001b[39m open_with_spark(\u001b[39m\"\u001b[39m\u001b[39mdata/ratings.csv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39mhist(R\u001b[39m.\u001b[39;49mcollect())\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o79.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:393)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:402)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:424)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3709/0x0000000801f66cf8.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset$$Lambda$3660/0x0000000801f2d168.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.Dataset$$Lambda$2086/0x0000000801b944f0.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset$$Lambda$1753/0x0000000801abd458.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1764/0x0000000801ac0d48.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1754/0x0000000801abd718.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeVirtual(DirectMethodHandle$Holder)\n\tat java.base/java.lang.invoke.LambdaForm$MH/0x0000000801160400.invoke(LambdaForm$MH)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "R = open_with_spark(\"data/ratings.csv\").select(\"rating\")\n",
    "plt.hist(R.collect())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Join Dataframes READABLE version, more memory needed\n",
    "def join_ratings_and_movies_readable():  \n",
    "    df_movie = open_with_spark()\n",
    "    df_ratings = open_with_spark(log_file=\"data/ratings.csv\", app_name=\"ratings\")\n",
    "    df_join = df_ratings.join(df_movie, \"movieID\")\n",
    "    return df_join\n",
    "    \n",
    "# 2. Join Dataframes\n",
    "def join_ratings_and_movies(): \n",
    "    return open_with_spark().join(open_with_spark(log_file=\"data/ratings.csv\", app_name=\"ratings\"), \"movieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 18:10:41 WARN Utils: Your hostname, Anthonys-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.67.104.86 instead (on interface en0)\n",
      "23/04/18 18:10:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 18:10:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/18 18:10:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+------+------+----------+\n",
      "|movieId|               title|        genres|userId|rating| timestamp|\n",
      "+-------+--------------------+--------------+------+------+----------+\n",
      "|    307|Three Colors: Blu...|         Drama|     1|   3.5|1256677221|\n",
      "|    481|   Kalifornia (1993)|Drama|Thriller|     1|   3.5|1256677456|\n",
      "|   1091|Weekend at Bernie...|        Comedy|     1|   1.5|1256677471|\n",
      "|   1257|Better Off Dead.....|Comedy|Romance|     1|   4.5|1256677460|\n",
      "|   1449|Waiting for Guffm...|        Comedy|     1|   4.5|1256677264|\n",
      "+-------+--------------------+--------------+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 18:10:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "X = join_ratings_and_movies()\n",
    "X.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 17:40:44 WARN MemoryStore: Not enough space to cache rdd_32_1 in memory! (computed 35.3 MiB so far)\n",
      "23/04/13 17:40:44 WARN BlockManager: Persisting block rdd_32_1 to disk instead.\n",
      "23/04/13 17:40:50 WARN MemoryStore: Not enough space to cache rdd_32_7 in memory! (computed 54.6 MiB so far)\n",
      "23/04/13 17:40:50 WARN BlockManager: Persisting block rdd_32_7 to disk instead.\n",
      "23/04/13 17:40:50 WARN MemoryStore: Not enough space to cache rdd_32_2 in memory! (computed 54.4 MiB so far)\n",
      "23/04/13 17:40:50 WARN BlockManager: Persisting block rdd_32_2 to disk instead.\n",
      "23/04/13 17:40:50 WARN MemoryStore: Not enough space to cache rdd_32_6 in memory! (computed 54.7 MiB so far)\n",
      "23/04/13 17:40:50 WARN BlockManager: Persisting block rdd_32_6 to disk instead.\n",
      "23/04/13 17:40:50 WARN MemoryStore: Not enough space to cache rdd_32_4 in memory! (computed 54.6 MiB so far)\n",
      "23/04/13 17:40:50 WARN BlockManager: Persisting block rdd_32_4 to disk instead.\n",
      "23/04/13 17:40:51 WARN MemoryStore: Not enough space to cache rdd_32_3 in memory! (computed 54.4 MiB so far)\n",
      "23/04/13 17:40:51 WARN BlockManager: Persisting block rdd_32_3 to disk instead.\n",
      "23/04/13 17:40:54 WARN MemoryStore: Not enough space to cache rdd_32_3 in memory! (computed 54.4 MiB so far)\n",
      "23/04/13 17:40:55 WARN MemoryStore: Not enough space to cache rdd_32_2 in memory! (computed 13.0 MiB so far)\n",
      "23/04/13 17:40:56 WARN MemoryStore: Not enough space to cache rdd_32_1 in memory! (computed 3.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(title='Shawshank Redemption, The (1994)', Num_ratings=97999)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated = X.groupby(\"title\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\").sort(\"Num_ratings\", ascending=False)\n",
    "df_updated.limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the top N movies with the most reviews (ratings)\n",
    "\n",
    "\"\"\"\n",
    "# (may be necessary for @param type)\n",
    "import pyspark\n",
    "\n",
    "# 3. Most-rated movies\n",
    "def most_rated(df: pyspark.sql.dataframe.DataFrame, N=10):\n",
    "    if N == None:\n",
    "        return df.groupby(\"title\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\").sort(\"Num_ratings\", ascending=False)\n",
    "    else:\n",
    "        return df.groupby(\"title\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\").sort(\"Num_ratings\", ascending=False).limit(N)\n",
    "\n",
    "# Function that does everything; can be used for timing or outputting or whatever\n",
    "def most_rated_complete(N=10):\n",
    "    JOINED = join_ratings_and_movies()\n",
    "    TOP_N = most_rated(JOINED, N)\n",
    "    return TOP_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               title|Num_ratings|\n",
      "+--------------------+-----------+\n",
      "|Shawshank Redempt...|      97999|\n",
      "| Forrest Gump (1994)|      97040|\n",
      "| Pulp Fiction (1994)|      92406|\n",
      "|Silence of the La...|      87899|\n",
      "|  Matrix, The (1999)|      84545|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_rated(X, 5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'title', 'genres', 'userId', 'rating', 'timestamp']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_df = most_rated(X, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 18:09:18 WARN MemoryStore: Not enough space to cache rdd_35_4 in memory! (computed 22.6 MiB so far)\n",
      "23/04/18 18:09:18 WARN MemoryStore: Not enough space to cache rdd_35_3 in memory! (computed 22.6 MiB so far)\n",
      "23/04/18 18:09:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/18 18:09:18 WARN MemoryStore: Not enough space to cache rdd_35_2 in memory! (computed 22.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_rated_df.repartition(1).write.csv('results/500_most_rated.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the top N movies with the highest average reviews (ratings)\n",
    "\n",
    "\"\"\"\n",
    "# (may be necessary for @param type)\n",
    "import pyspark\n",
    "\n",
    "# 4. Highest-average-rated movies\n",
    "def best_average_rated(df: pyspark.sql.dataframe.DataFrame, N=10, MIN_RATINGS=50):\n",
    "    T = df.groupby(\"title\").agg(F.mean(\"rating\"))\n",
    "    H = T.withColumnRenamed(\"avg(rating)\", \"Mean_rating\")\n",
    "    # H.show(5)\n",
    "    G = df.groupby(\"title\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\")\n",
    "    C = H.join(G, \"title\")\n",
    "    C = C.filter(C.Num_ratings >= MIN_RATINGS).select([\"title\", \"Mean_rating\"])\n",
    "    J = C.sort(\"Mean_rating\", ascending=False)\n",
    "    K = J.limit(N).withColumn(\"Mean_rating\", F.round(\"Mean_rating\",3))\n",
    "    return K\n",
    "\n",
    "# Function that does everything; can be used for timing or outputting or whatever\n",
    "def best_average_rated_complete(N=10, MIN_RATINGS=50):\n",
    "    JOINED = join_ratings_and_movies()\n",
    "    TOP_N = best_average_rated(JOINED, N, MIN_RATINGS)\n",
    "    return TOP_N #.select(\"*\", round(\"Mean_ratings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:07 WARN Utils: Your hostname, Coopers-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.16.108.70 instead (on interface en0)\n",
      "23/04/14 16:21:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/14 16:21:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                  (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:18 WARN BlockManager: Block rdd_23_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                  (0 + 8) / 8][Stage 5:>                  (0 + 0) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:45 WARN MemoryStore: Not enough space to cache rdd_37_1 in memory! (computed 54.3 MiB so far)\n",
      "23/04/14 16:21:45 WARN BlockManager: Persisting block rdd_37_1 to disk instead.\n",
      "23/04/14 16:21:45 WARN MemoryStore: Not enough space to cache rdd_37_2 in memory! (computed 54.4 MiB so far)\n",
      "23/04/14 16:21:45 WARN BlockManager: Persisting block rdd_37_2 to disk instead.\n",
      "23/04/14 16:21:45 WARN MemoryStore: Not enough space to cache rdd_37_0 in memory! (computed 54.4 MiB so far)\n",
      "23/04/14 16:21:45 WARN BlockManager: Persisting block rdd_37_0 to disk instead.\n",
      "23/04/14 16:21:45 WARN MemoryStore: Not enough space to cache rdd_37_4 in memory! (computed 54.6 MiB so far)\n",
      "23/04/14 16:21:45 WARN BlockManager: Persisting block rdd_37_4 to disk instead.\n",
      "23/04/14 16:21:45 WARN MemoryStore: Not enough space to cache rdd_37_3 in memory! (computed 54.4 MiB so far)\n",
      "23/04/14 16:21:45 WARN BlockManager: Persisting block rdd_37_3 to disk instead.\n",
      "23/04/14 16:21:46 WARN MemoryStore: Not enough space to cache rdd_37_6 in memory! (computed 54.7 MiB so far)\n",
      "23/04/14 16:21:46 WARN BlockManager: Persisting block rdd_37_6 to disk instead.\n",
      "23/04/14 16:21:46 WARN MemoryStore: Not enough space to cache rdd_37_5 in memory! (computed 54.7 MiB so far)\n",
      "23/04/14 16:21:46 WARN BlockManager: Persisting block rdd_37_5 to disk instead.\n",
      "23/04/14 16:21:46 WARN MemoryStore: Not enough space to cache rdd_37_7 in memory! (computed 54.6 MiB so far)\n",
      "23/04/14 16:21:46 WARN BlockManager: Persisting block rdd_37_7 to disk instead.\n",
      "23/04/14 16:21:52 WARN MemoryStore: Not enough space to cache rdd_37_1 in memory! (computed 12.9 MiB so far)\n",
      "23/04/14 16:21:52 WARN MemoryStore: Not enough space to cache rdd_37_2 in memory! (computed 54.4 MiB so far)\n",
      "23/04/14 16:21:53 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_37_0 in memory.\n",
      "23/04/14 16:21:53 WARN MemoryStore: Not enough space to cache rdd_37_0 in memory! (computed 384.0 B so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==>                (1 + 7) / 8][Stage 5:>                  (0 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:56 WARN MemoryStore: Not enough space to cache rdd_37_0 in memory! (computed 13.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========>       (5 + 3) / 8][Stage 5:>                  (0 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:57 WARN MemoryStore: Not enough space to cache rdd_37_1 in memory! (computed 54.3 MiB so far)\n",
      "23/04/14 16:21:57 WARN MemoryStore: Not enough space to cache rdd_37_2 in memory! (computed 54.4 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:================>  (7 + 1) / 8][Stage 5:>                  (0 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 16:21:59 WARN MemoryStore: Not enough space to cache rdd_37_7 in memory! (computed 22.6 MiB so far)\n",
      "23/04/14 16:21:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/04/14 16:21:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               title|Mean_rating|\n",
      "+--------------------+-----------+\n",
      "|Planet Earth II (...|      4.487|\n",
      "| Planet Earth (2006)|      4.458|\n",
      "|Shawshank Redempt...|      4.424|\n",
      "|Band of Brothers ...|        4.4|\n",
      "|Black Mirror: Whi...|      4.351|\n",
      "|              Cosmos|      4.344|\n",
      "|The Godfather Tri...|       4.34|\n",
      "|Godfather, The (1...|      4.333|\n",
      "|Usual Suspects, T...|      4.292|\n",
      "|        Black Mirror|      4.264|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "A = best_average_rated_complete()\n",
    "A.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Popular genres: Find the top N popular genres by calculating the average rating for each genre.\n",
    "\"\"\"\n",
    "import pyspark\n",
    "def popular_genres(df: pyspark.sql.dataframe.DataFrame, N=5, MIN_RATINGS=10):\n",
    "    L = df.groupby(\"genres\").agg(F.mean(\"rating\"))\n",
    "    L.show(5)\n",
    "    M = L.withColumnRenamed(\"avg(rating)\", \"Mean_rating\")\n",
    "    G = df.groupby(\"genres\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\")\n",
    "    M.show(5)\n",
    "    K = M.join(G, \"genres\")\n",
    "    P = K.filter(K.Num_ratings >= MIN_RATINGS).select([\"genres\", \"Mean_rating\"])\n",
    "    N = P.sort(\"Mean_rating\", ascending=False).limit(N).withColumn(\"Mean_rating\", F.round(\"Mean_rating\",3))\n",
    "    return N\n",
    "def popular_genres_complete(N=5, MIN_RATINGS=10):\n",
    "    JOINED = join_ratings_and_movies()\n",
    "    TOP_N = popular_genres(JOINED, N)\n",
    "    return TOP_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 18:02:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/04/16 18:02:52 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/16 18:02:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/04/16 18:02:53 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/16 18:02:53 WARN MemoryStore: Not enough space to cache rdd_32_0 in memory! (computed 22.6 MiB so far)\n",
      "23/04/16 18:02:53 WARN MemoryStore: Not enough space to cache rdd_32_2 in memory! (computed 22.5 MiB so far)\n",
      "23/04/16 18:02:53 WARN MemoryStore: Not enough space to cache rdd_32_1 in memory! (computed 35.3 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              genres|       avg(rating)|\n",
      "+--------------------+------------------+\n",
      "|Comedy|Horror|Thr...| 3.288320727995902|\n",
      "|Adventure|Sci-Fi|...| 3.212121212121212|\n",
      "|Action|Adventure|...| 4.011721534573262|\n",
      "| Action|Drama|Horror|3.7695176529090006|\n",
      "|Action|Animation|...|  3.76522506619594|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "23/04/16 18:02:57 WARN MemoryStore: Not enough space to cache rdd_32_1 in memory! (computed 22.5 MiB so far)\n",
      "23/04/16 18:02:57 WARN MemoryStore: Not enough space to cache rdd_32_0 in memory! (computed 22.6 MiB so far)\n",
      "23/04/16 18:02:57 WARN MemoryStore: Not enough space to cache rdd_32_2 in memory! (computed 35.2 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|              genres|       Mean_rating|\n",
      "+--------------------+------------------+\n",
      "|Comedy|Horror|Thr...| 3.288320727995902|\n",
      "|Adventure|Sci-Fi|...| 3.212121212121212|\n",
      "|Action|Adventure|...| 4.011721534573262|\n",
      "| Action|Drama|Horror|3.7695176529090006|\n",
      "|Action|Animation|...|  3.76522506619594|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "23/04/16 18:03:02 WARN MemoryStore: Not enough space to cache rdd_32_0 in memory! (computed 22.6 MiB so far)\n",
      "23/04/16 18:03:02 WARN MemoryStore: Not enough space to cache rdd_32_1 in memory! (computed 22.5 MiB so far)\n",
      "23/04/16 18:03:02 WARN MemoryStore: Not enough space to cache rdd_32_2 in memory! (computed 35.2 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===========>      (5 + 3) / 8][Stage 42:>                 (0 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 18:03:04 WARN MemoryStore: Not enough space to cache rdd_32_1 in memory! (computed 22.5 MiB so far)\n",
      "23/04/16 18:03:04 WARN MemoryStore: Not enough space to cache rdd_32_0 in memory! (computed 35.3 MiB so far)\n",
      "23/04/16 18:03:04 WARN MemoryStore: Not enough space to cache rdd_32_2 in memory! (computed 22.5 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|              genres|Mean_rating|\n",
      "+--------------------+-----------+\n",
      "|Action|Adventure|...|      4.201|\n",
      "|Film-Noir|Romance...|      4.164|\n",
      "|Action|Crime|Dram...|      4.163|\n",
      "|Action|Adventure|...|      4.157|\n",
      "|Action|Crime|Dram...|      4.156|\n",
      "|Adventure|Animati...|      4.152|\n",
      "|Animation|Childre...|      4.145|\n",
      "|   Film-Noir|Mystery|      4.128|\n",
      "|Crime|Film-Noir|M...|      4.127|\n",
      "|Action|Adventure|...|       4.12|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = popular_genres_complete(10)\n",
    "W.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Year-wise analysis: \n",
    "\n",
    "Extract the release year from the movie title and analyze the number of movies released and their average ratings per year.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# code needs some work filtering out bad years, works in general but some title must not end with the year in ( )\n",
    "def year_analysis(df: pyspark.sql.dataframe.DataFrame, N=10):\n",
    "    # add year column\n",
    "    J = df.withColumn(\"year\", F.col(\"title\").substr(F.length(\"title\")-4, F.length(\"title\")).substr(1,4)).select(\"movieID\", \"year\")\n",
    "    K = J.join(open_with_spark(\"data/ratings.csv\").select(\"movieID\", \"rating\"), \"movieID\")\n",
    "    #K.show(200)\n",
    "    # get average rating per year\n",
    "    #D = K.groupby(\"year\").agg(F.mean(\"rating\")).withColumnRenamed(\"avg(rating)\", \"Mean_rating\").sort(\"Mean_rating\", ascending = False)\n",
    "    L = K.groupby(\"year\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\").sort(\"Num_ratings\", ascending=False)\n",
    "    \n",
    "    return  L #K.join(D, \"year\")\n",
    "def year_analysis_complete(N=10):\n",
    "    X = open_with_spark()\n",
    "    Y = year_analysis(X, N)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 14:33:14 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/27 14:33:14 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|Num_ratings|\n",
      "+----+-----------+\n",
      "|1995|    1767979|\n",
      "|1994|    1529657|\n",
      "|1996|    1334834|\n",
      "|1999|    1305889|\n",
      "|2000|    1089609|\n",
      "|1993|    1082362|\n",
      "|1997|    1080028|\n",
      "|1998|    1040732|\n",
      "|2001|     983000|\n",
      "|2002|     885726|\n",
      "|2004|     849291|\n",
      "|2003|     779270|\n",
      "|2006|     617677|\n",
      "|2007|     579220|\n",
      "|2005|     570873|\n",
      "|1992|     567142|\n",
      "|1989|     558507|\n",
      "|1990|     548939|\n",
      "|2008|     547974|\n",
      "|2009|     507131|\n",
      "|1991|     486961|\n",
      "|2010|     456500|\n",
      "|1988|     428412|\n",
      "|1987|     424588|\n",
      "|1986|     422299|\n",
      "|1984|     403551|\n",
      "|2011|     372899|\n",
      "|2012|     362019|\n",
      "|2014|     356580|\n",
      "|1985|     348500|\n",
      "|2013|     336980|\n",
      "|1982|     303667|\n",
      "|2015|     278900|\n",
      "|1980|     270448|\n",
      "|1983|     245945|\n",
      "|1981|     245589|\n",
      "|1979|     220638|\n",
      "|2016|     204613|\n",
      "|1975|     182415|\n",
      "|1977|     170660|\n",
      "|1971|     153949|\n",
      "|1978|     136967|\n",
      "|1973|     132878|\n",
      "|1974|     130366|\n",
      "|1968|     125388|\n",
      "|2017|     122004|\n",
      "|1976|     121598|\n",
      "|1972|     118265|\n",
      "|1964|     108186|\n",
      "|1967|      99821|\n",
      "|1962|      86824|\n",
      "|1963|      80458|\n",
      "|1959|      80382|\n",
      "|1954|      79788|\n",
      "|1960|      76628|\n",
      "|1957|      69140|\n",
      "|1961|      68828|\n",
      "|1970|      67528|\n",
      "|1939|      67043|\n",
      "|1940|      66723|\n",
      "|1969|      62521|\n",
      "|1965|      60393|\n",
      "|1941|      59875|\n",
      "|1966|      55574|\n",
      "|1955|      55313|\n",
      "|1951|      55084|\n",
      "|1958|      54603|\n",
      "|1942|      53948|\n",
      "|1953|      48457|\n",
      "|1950|      47618|\n",
      "|1946|      42872|\n",
      "|1956|      39385|\n",
      "|1937|      31128|\n",
      "|1952|      30217|\n",
      "|1944|      29174|\n",
      "|1948|      26174|\n",
      "|2018|      23329|\n",
      "|1949|      21395|\n",
      "|1931|      20997|\n",
      "|1933|      19907|\n",
      "|1938|      18922|\n",
      "|1947|      16938|\n",
      "|1935|      15292|\n",
      "|1945|      14924|\n",
      "|1934|      14394|\n",
      "|1936|      12907|\n",
      "|1932|       9612|\n",
      "|1943|       9496|\n",
      "|1927|       9120|\n",
      "|1925|       7214|\n",
      "|1930|       6175|\n",
      "|1922|       5381|\n",
      "|1928|       4352|\n",
      "|1926|       3883|\n",
      "|1929|       3136|\n",
      "|1920|       2426|\n",
      "|1921|       1962|\n",
      "|1924|       1918|\n",
      "|ligh|       1707|\n",
      "|imal|       1341|\n",
      "|1923|       1208|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "A = year_analysis_complete()\n",
    "A = A.filter(A.Num_ratings > 1106)\n",
    "A.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Outputs the users who rate the most\n",
    "\n",
    "N is the number of top users to output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def user_analysis_top_users(df: pyspark.sql.dataframe.DataFrame, N=10):\n",
    "    Y = df.groupby(\"userID\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\").sort(\"Num_ratings\", ascending=False).limit(N)\n",
    "    Z = df.groupby(\"userID\").agg(F.mean(\"rating\")).withColumnRenamed(\"avg(rating)\", \"Mean_ratings\")\n",
    "    return Y.join(Z,\"userID\")\n",
    "def user_analysis_top_users_complete(N=10):\n",
    "    X = open_with_spark(log_file=\"data/ratings.csv\")\n",
    "    Y = user_analysis_top_users(X, N)\n",
    "    return Y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Outputs the users who rate the HIGHEST (on average)\n",
    "\n",
    "N is the number of top users to output\n",
    "MIN is the minimum number of ratings to be considered (if not min all 5's)\n",
    "\n",
    "ASCENDING is boolean to make function the LOWEST (on average) raters\n",
    "\n",
    "\"\"\"   \n",
    "def user_analysis_avg_raters(df: pyspark.sql.dataframe.DataFrame, N=10, MIN=100, ASCENDING=False):\n",
    "    Y = df.groupby(\"userID\").agg(F.mean(\"rating\")).withColumnRenamed(\"avg(rating)\", \"Mean_ratings\")\n",
    "    Z = df.groupby(\"userID\").agg(F.count(\"rating\")).withColumnRenamed(\"count(rating)\", \"Num_ratings\")\n",
    "    Z = Z.filter(Z.Num_ratings >= MIN)\n",
    "    \n",
    "    return Y.join(Z,\"userID\").sort(\"Mean_ratings\", ascending=ASCENDING).limit(N)\n",
    "\n",
    "def user_analysis_avg_raters_complete(N=10, MIN=100, ASCENDING=False):\n",
    "    X = open_with_spark(log_file=\"data/ratings.csv\")\n",
    "    Y = user_analysis_avg_raters(X, N, MIN, ASCENDING)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 14:54:10 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------+\n",
      "|userID|     Mean_ratings|Num_ratings|\n",
      "+------+-----------------+-----------+\n",
      "|165601|              5.0|        505|\n",
      "|236710|              5.0|        518|\n",
      "|234304|4.999380421313507|        807|\n",
      "| 46846| 4.99009900990099|        505|\n",
      "| 42161|            4.966|       1750|\n",
      "|101046|4.893877551020408|        980|\n",
      "|107894|4.836801040312094|        769|\n",
      "|256715|4.836029411764706|        680|\n",
      "|160393|4.824519230769231|        832|\n",
      "| 81072|4.797959183673469|        735|\n",
      "+------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = user_analysis_avg_raters_complete(MIN=500)\n",
    "D.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af4521a1c734e7c65889f479bb79ece66303508270e0aa19c37418c08bff644d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
